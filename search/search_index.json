{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#_1","title":"Home","text":"G\u00d7E\u00d7M Innovation in Intelligence for Climate Adaptation <p>Our Mission</p> <p>Many staple crops that are important for food, nutritional, and economic security in low and middle-income countries have not experienced the same large gains in yield and quality over last decades as crops such as maize and soybean. Further, these crops are faced with increasing risk and uncertain growing conditions due to climate change. This project aims to develop a state-of-the-art breeding toolkit, building on the latest techniques in AI-enabled sensing, 3-D crop modeling, and molecular breeding, to create an inflection point in the productivity and quality curves of crops that are central in LMICs.</p> <p>The sensing team is led by Dr. Mason Earles and will focus on using artificial intelligence and machine learning to speed up the phenotyping process in new crop varieties. While using cameras and sensors to select or measure desirable traits is nothing new in agriculture, the use of AI-enabled sensors will cut down on the time and cost of labor, which will help accelerate the development of stress-resistant, nutritious staple crops.</p> <p>To accomplish this goal of accelerating crop development, the team is implementing a variety of sensing modalities for data collection, such as color, infrared, and thermal imagery as well as LiDAR scanning from both ground-based and UAV-mounted sensors. Additionally, both high quality as well as affordable sensors are being deployed with the goal of extracting the most information possible from sensors which can be deployed with minimal cost. With this data, along with the ground truth data collected by the Pre-breeding &amp; Genomics team, our goal is to validate AI-based models for measurement of breeding traits such as stand count, plant height, and flowering time, as well as to develop models for traits which would otherwise require extensive labor to collect across an entire field, such as leaf geometry, flower count, and yield estimation at regular intervals throughout the growing season. While rapid collection of phenotypic data across a breeding population will accelerate the development of improved crop varieties on its own, this high-resolution trait data will also inform the generation of high-quality 3D models by the 3D Modeling team. These 3D models can be used to increase the quality of AI-based models, further improving the quality of phenotyping data without the need for laborious ground truth data collection by small-scale producers and breeders.</p> <p></p>"},{"location":"1.%20App/1-%20Installation/","title":"Start Guide","text":""},{"location":"1.%20App/1-%20Installation/#installation-guide","title":"Installation Guide","text":""},{"location":"1.%20App/1-%20Installation/#quick-start-with-docker-compose-recommended","title":"Quick Start with Docker Compose (Recommended)","text":"<p>The easiest way to get started with the GEMINI App is using Docker Compose. This method automatically handles all dependencies and configurations.</p>"},{"location":"1.%20App/1-%20Installation/#prerequisites","title":"Prerequisites","text":"<p>Install Docker Desktop for your operating system:</p> <ul> <li>Windows</li> <li>MacOS</li> <li>Linux</li> </ul>"},{"location":"1.%20App/1-%20Installation/#installation-steps","title":"Installation Steps","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/GEMINI-Breeding/GEMINI-App.git\ncd GEMINI-App\n</code></pre> </li> <li> <p>Create the data directory:</p> <pre><code>mkdir ~/GEMINI-App-Data\n</code></pre> </li> <li> <p>Optional: Customize the environment variables</p> <p>Open the <code>.env</code> file in the <code>GEMINI-App</code> directory to customize ports and data directory</p> <pre><code># .env file for GEMINI-App\n# MapBox Access Token (required for map functionality)\nREACT_APP_MAPBOX_TOKEN=your.mapbox.access.token.here\n\n# Port Configuration\nREACT_APP_FRONT_PORT=3000\nREACT_APP_FLASK_PORT=5050\nREACT_APP_TILE_SERVER_PORT=8091\nPORT=$REACT_APP_FRONT_PORT\n\n# Application Data Directory (absolute path)\nREACT_APP_APP_DATA=$HOME/GEMINI-App-Data\n</code></pre> <p>Notes:</p> <ul> <li>Get your MapBox token from MapBox Access Tokens</li> <li>Comments must be on separate lines (inline comments are not supported)</li> <li>Use absolute paths for <code>REACT_APP_APP_DATA</code></li> <li>Default values will be used if <code>.env</code> file is not present</li> </ul> </li> <li> <p>Run the application</p> <pre><code># CPU version (default)\ndocker compose up --pull always\n\n# GPU version (if nvidia-smi works on host)\ndocker compose -f docker-compose-gpu.yml up --pull always\n</code></pre> <p>Note: The <code>--pull always</code> flag ensures you're always using the latest Docker images.</p> </li> <li> <p>Access the application:</p> <ul> <li>Frontend: http://localhost:3000 (or your configured <code>REACT_APP_FRONT_PORT</code>)</li> </ul> <p>For debugging:</p> <ul> <li>Backend API: http://localhost:5050 (or your configured <code>REACT_APP_FLASK_PORT</code>)</li> <li>Tile Server: http://localhost:8091 (or your configured <code>REACT_APP_TILE_SERVER_PORT</code>)</li> </ul> </li> </ol>"},{"location":"1.%20App/1-%20Installation/#docker-configuration","title":"Docker Configuration","text":""},{"location":"1.%20App/1-%20Installation/#understanding-docker-composeyml","title":"Understanding docker-compose.yml","text":"<p>The <code>docker-compose.yml</code> file defines how the GEMINI App runs in Docker. Here's what each section does:</p> <pre><code>services:\n  app:\n    image: paibl/gemini-breeding:latest  # Pre-built Docker image\n    build:\n      context: .\n      dockerfile: Dockerfile\n    env_file:\n      - ./gemini-app/.env  # Load environment variables\n    ports:\n      - \"${REACT_APP_FRONT_PORT:-3000}:${REACT_APP_FRONT_PORT:-3000}\"   # Frontend\n      - \"${REACT_APP_FLASK_PORT:-5050}:${REACT_APP_FLASK_PORT:-5050}\"   # Backend\n      - \"${REACT_APP_TILE_SERVER_PORT:-8091}:${REACT_APP_TILE_SERVER_PORT:-8091}\"  # Tile Server\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock  # Allow Docker-in-Docker\n      - ${REACT_APP_APP_DATA:-~/GEMINI-App-Data}:/root/GEMINI-App-Data  # Data directory\n      - ./gemini-app/.env:/app/gemini-app/.env:ro  # Environment variables (read-only)\n</code></pre> <p>Key Configuration Parameters:</p> <ul> <li> <p>Ports: Maps host ports to container ports</p> <ul> <li>Format: <code>\"host_port:container_port\"</code></li> <li>Default ports: 3000 (frontend), 5050 (backend), 8091 (tile server)</li> <li>Change these if ports are already in use on your system</li> </ul> </li> <li> <p>Volumes:</p> <ul> <li>Docker socket: <code>/var/run/docker.sock</code> enables Docker-in-Docker for OpenDroneMap</li> <li>Data directory: <code>~/GEMINI-App-Data</code> stores all application data (images, orthomosaics, models)</li> <li>Environment file: <code>.env</code> file is mounted as read-only</li> </ul> </li> <li> <p>Environment Variables: Loaded from <code>.env</code> file with fallback defaults</p> </li> </ul> <p>For more detailed configuration options, see the docker-compose.yml file.</p>"},{"location":"1.%20App/1-%20Installation/#updating-the-application","title":"Updating the Application","text":"<p>To update to the latest version:</p> <pre><code>cd GEMINI-App\ndocker compose down\ndocker compose up --pull always\n</code></pre> <p>The <code>--pull always</code> flag will automatically download the latest Docker images before starting the containers.</p>"},{"location":"1.%20App/1-%20Installation/#rebuilding-docker-images","title":"Rebuilding Docker Images","text":"<p>If you've made local changes and need to rebuild:</p> <pre><code># Rebuild and start\ndocker compose up --build\n\n# Or rebuild specific services\ndocker compose build\ndocker compose up\n</code></pre>"},{"location":"1.%20App/1-%20Installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"1.%20App/1-%20Installation/#windows","title":"Windows","text":"<ul> <li>Install WSL2 for better Docker performance. Install the default Ubuntu distribution to avoid compatibility issues.</li> <li>Ensure Docker Desktop is configured to use WSL2 backend.</li> <li>Follow the instructions for Configuring WSL2 with Docker Desktop.</li> <li>The data directory path in WSL2: <code>~/GEMINI-App-Data</code> (equivalent to <code>/home/yourusername/GEMINI-App-Data</code>)</li> <li>In your <code>.env</code> file, use Linux-style paths when running in WSL2</li> </ul>"},{"location":"1.%20App/1-%20Installation/#macos","title":"MacOS","text":"<ul> <li>Install XCode and CLI Tools for development.</li> <li>Note: The GEMINI App attempts to find an NVIDIA GPU for OpenDroneMap orthomosaic generation. Due to the lack of a compatible GPU on MacOS systems, the following error is expected:   <code>docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].</code>   The app will automatically fall back to CPU processing, which may result in longer processing times.</li> </ul>"},{"location":"1.%20App/1-%20Installation/#linux","title":"Linux","text":"<ul> <li>The recommended distribution for the GEMINI App is Ubuntu. Other distros may encounter compatibility issues.</li> <li>If your system has an NVIDIA GPU for orthophoto generation:</li> <li>Install the NVIDIA Container Toolkit</li> <li>Follow the relevant setup instructions</li> <li>Use the GPU-enabled Docker Compose file:</li> </ul> <pre><code>docker compose -f docker-compose-gpu.yml up --pull always\n</code></pre> <ul> <li>If the NVIDIA Container Toolkit is not installed, the following error will appear during orthophoto generation:</li> </ul> <pre><code>docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\n</code></pre> <p>The app will fall back to CPU processing automatically.</p>"},{"location":"1.%20App/1-%20Installation/#advanced-native-installation-for-developers","title":"Advanced: Native Installation for Developers","text":"<p>For developers who want to run the application natively without Docker:</p>"},{"location":"1.%20App/1-%20Installation/#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Git</li> <li>Python 3.8+</li> <li>Node.js 18</li> <li>Docker Desktop (for OpenDroneMap container)</li> </ul>"},{"location":"1.%20App/1-%20Installation/#installation-steps_1","title":"Installation Steps","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone https://github.com/GEMINI-Breeding/GEMINI-App.git\ncd GEMINI-App\n</code></pre> </li> <li> <p>Initialize git submodules:</p> <pre><code>git submodule update --init --recursive\n</code></pre> </li> <li> <p>Set up Flask backend:</p> <pre><code>cd GEMINI-Flask-Server\n./install_flask_server.sh\ncd ../\n</code></pre> </li> <li> <p>Install Node Version Manager (NVM):</p> <pre><code>curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.5/install.sh | bash\nsource ~/.bashrc\n</code></pre> </li> <li> <p>Install Node 18:</p> <pre><code>nvm install 18\nnvm use 18\n</code></pre> </li> <li> <p>Install frontend dependencies:</p> <pre><code>cd gemini-app\nnpm install --legacy-peer-deps  # Fix upstream dependency conflicts\n</code></pre> </li> </ol>"},{"location":"1.%20App/1-%20Installation/#running-the-app","title":"Running the App","text":"<p>Ensure Docker Desktop is running before starting the GEMINI App.</p> <p>In <code>GEMINI-App/gemini-app</code>:</p> <pre><code># Run development server (frontend and backend concurrently)\nnpm run gemini \n\n# Run frontend only\nnpm run front\n\n# Run Flask server only\nnpm run server\n</code></pre>"},{"location":"1.%20App/1-%20Installation/#updating-native-installation","title":"Updating Native Installation","text":""},{"location":"1.%20App/1-%20Installation/#option-1-auto-update-using-startup-script","title":"Option 1: Auto-Update using startup script","text":"<pre><code>cd GEMINI-App\n./startup.sh\n</code></pre>"},{"location":"1.%20App/1-%20Installation/#option-2-manual-update-using-git","title":"Option 2: Manual update using git","text":"<pre><code>cd GEMINI-App\n\n# Stash any local changes\ngit stash\ncd GEMINI-Flask-Server\ngit stash\ncd ..\n\n# Pull changes\ngit fetch\ngit pull\n\n# Update submodules\ngit submodule update --init --recursive\n</code></pre>"},{"location":"1.%20App/1-%20Installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"1.%20App/1-%20Installation/#docker-related-issues","title":"Docker-Related Issues","text":"<p>Container fails to start: </p> <ul> <li>Ensure Docker Desktop is running and has sufficient resources allocated (recommended: 4GB RAM minimum)</li> <li>Check if <code>.env</code> file is properly formatted (no inline comments)</li> </ul> <p>Port conflicts:    - If ports 3000, 5050, or 8091 are already in use, modify the port mappings in <code>.env</code> file   - Example: Change <code>REACT_APP_FRONT_PORT=3000</code> to <code>REACT_APP_FRONT_PORT=3050</code></p> <p>Volume mount issues: </p> <ul> <li>Verify that <code>~/GEMINI-App-Data</code> exists and has proper permissions</li> <li>Check that the path in <code>.env</code> file is absolute, not relative</li> <li>On Windows WSL2, ensure you're using Linux-style paths</li> </ul> <p>Environment variable not loading:</p> <ul> <li>Ensure <code>.env</code> file is in the correct location (<code>GEMINI-App/</code> directory)</li> <li>Check for syntax errors (comments must be on separate lines)</li> <li>Restart containers after modifying <code>.env</code>: <code>docker compose down &amp;&amp; docker compose up</code></li> </ul>"},{"location":"1.%20App/1-%20Installation/#native-installation-issues","title":"Native Installation Issues","text":"<p>Failed to upload file error: </p> <ul> <li>Check that the path in <code>package.json</code> points to an existing and accessible <code>GEMINI-App-Data</code> directory</li> <li>Verify the Flask Server started successfully</li> <li>Check terminal logs for Flask Server errors</li> </ul> <p>Terminal cleared during npm start: </p> <ul> <li>If you cannot scroll up to see previous commands after running <code>npm start gemini</code>, Docker is not running properly</li> <li>Restart Docker Desktop and try again</li> </ul> <p>Orthophoto generation failures: </p> <ul> <li>Manually delete the <code>~/GEMINI-App-Data/temp</code> directory</li> <li>Try generation again</li> <li>Check Docker logs for OpenDroneMap container errors</li> </ul> <p>farm-ng-core installation error (MacOS): </p> <ul> <li>If you see: <code>error: variable length arrays in C++ are a Clang extension [-Werror,-Wvla-cxx-extension]</code></li> <li>Open <code>setup.py</code> inside the <code>farm-ng-core</code> directory</li> <li>Remove <code>\"-Werror\"</code> from the <code>extra_compile_args</code> list</li> <li>Retry installation</li> </ul> <p>WSL2-specific issues (Windows):</p> <ul> <li>Install libgl1 library: <code>sudo apt-get update &amp;&amp; sudo apt-get install libgl1</code></li> <li>Add user to docker group: <code>sudo usermod -aG docker $USER</code></li> <li>Apply changes: <code>newgrp docker</code></li> <li>Verify Docker context: <code>docker context use default</code></li> </ul>"},{"location":"1.%20App/1-%20Installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check the Full Documentation</li> <li>Review GitHub Issues</li> <li>Create a new issue with detailed error logs and system information</li> </ol>"},{"location":"1.%20App/2-%20Amiga/","title":"Amiga Data Processing","text":""},{"location":"1.%20App/2-%20Amiga/#upload-and-extract-data","title":"Upload and Extract Data","text":"<p>To navigate to the File Upload page, click on the <code>Upload</code> Icon. In the Upload Files section, select <code>Amiga File</code> as the data type from the dropdown.</p>"},{"location":"1.%20App/2-%20Amiga/#data-fields","title":"Data Fields","text":"<p>Year: This is the year <code>YYYY</code> the data was collected. It is a required field. For example, <code>2022</code> is a valid year.</p> <p>Experiment: This is open for the user to decide. This will differentiate experiments for a single year. For example, <code>Experiment 1</code> is a valid experiment.</p> <p>Location: This is the location where the data was collected. For example, <code>Davis</code> is a valid location.</p> <p>Population: This is the type of plants you will be analyzing. For example, <code>Legumes</code> is a valid population.</p> <p>Date: This is the date <code>YYYY-MM-DD</code> the data was collected. It is a required field. For example, <code>2022-06-20</code> is a valid date.</p>"},{"location":"1.%20App/2-%20Amiga/#uploading-amiga-data","title":"Uploading Amiga Data","text":"<ul> <li>Fill in the data fields for the Amiga file(s) you wish to upload. </li> <li>Select the file(s) by dropping files in the dropzone or clicking the dropzone to open a file explorer popup.</li> <li>Alternatively, choose <code>Select Folder</code> if all binary files you wish to upload are in a single folder.</li> <li>The binary files will be uploaded to the app and extracted. Monitor the progress bar in the app to see the upload and extraction progress. For single-file extractions, the extraction progress bar is expected to jump from 0-100 when completed. For multiple files, the bar will update as each file is completed.</li> </ul>"},{"location":"1.%20App/2-%20Amiga/#prepare-data","title":"Prepare Data","text":"<p>Navigate to the Manage Files tab from the top of the File Upload page. Select <code>Amiga File</code> as the data type from the dropdown. </p>"},{"location":"1.%20App/2-%20Amiga/#data-table","title":"Data Table","text":"<ul> <li>Navigate to the <code>Manage Files</code> tab to view uploaded data and prepare for processing. </li> <li>Hover over any of the action buttons in the <code>Actions</code> column to see their function.</li> </ul>"},{"location":"1.%20App/2-%20Amiga/#plot-marking","title":"Plot Marking","text":"<ul> <li>For processing of Amiga data, plots must be marked in the file management tab before proceeding. </li> <li>Select <code>Amiga File</code> from the Data Type dropdown. </li> <li>Click the pin icon to open the plot marker.</li> </ul> <p>The plot marker is used to associate images and their GPS coordinates with distinct plots within your field and create stitched images of each plot. Plot marking is performed on an initial date including all plots within the experiment and future dates will be filtered based on these initial plot borders.</p> <ul> <li>The primary functionality of the plot marker is to mark the start and end of each plot. </li> <li>To begin, navigate to the start of the first plot. Click the start button or press enter on your keyboard. </li> <li>Use the arrow keys, WASD, or a mouse to iterate to the end of the plot (Up / Down = Jump Forward 10 Images / Jump Back 10 Images). </li> <li>Align the overlaid midline with the end of the plot and click the stop button (or press enter) to mark the plot end. </li> <li>On your first plot, a popup will appear asking for Stitch Direction. This direction is based on the position of the end image with respect to the start.</li> <li>If you mistakenly toggle the start button, click the purple cancel button to reset.</li> <li> <p>Hover over the icons present on the screen for information on the button's functionality.</p> </li> <li> <p>Plot Filtering: After marking all plots for a given date and associating plot labels / accessions with each, future dates will be filtered based on GPS data to automate the process. </p> <ul> <li>If the filtering does not appear to work, go to the GPS Data panel of the originally marked date. Choose a clear reference point (visible between dates, such as a flag or GCP) and click <code>Set as GPS Reference</code>. </li> <li>Re-open the plot marker of the newly extracted date. Open the GPS Data panel and click <code>Shift Plots</code>. After this completes, close and reopen the plot marker to reapply the filtering (filter button is present but not 100% functional as of now).</li> </ul> </li> </ul>"},{"location":"1.%20App/2-%20Amiga/#stitch-generation","title":"Stitch Generation","text":"<ul> <li>Open the process tab to create the stitched plots for your experiment. Populate the fields in the selection panel to select the correct year / location / experiment / population.</li> </ul>"},{"location":"1.%20App/2-%20Amiga/#agrowstitch","title":"AgRowStitch","text":"<ul> <li>In the Orthomosaic Generation tab, select <code>Start</code> on the date you wish to generate stitches for.</li> <li>Under the <code>Orthomosaic Method</code> dropdown, select AgRowStitch. This is recommended for ground-based data.</li> <li>For now, you can skip the GCP picking process and directly select <code>Generate Orthophoto</code>.</li> <li>Generation will begin and progress can be tracked via the progress bar at the bottom of the page. </li> <li>After generation is completed, navigate to the Generated Orthomosaics tab to view and download the stitched images.</li> </ul>"},{"location":"1.%20App/2-%20Amiga/#plot-boundary-preparation","title":"Plot Boundary Preparation","text":""},{"location":"1.%20App/2-%20Amiga/#import-data","title":"Import Data","text":"<ul> <li>After stitch generation is finished, the field's plot and accession information is associated with the plot indexes from the plot marker.</li> <li>Navigate to the Plot Boundary Preparation tab at the top of the screen.</li> <li>First, input a field_design.csv file in the requested format:</li> </ul> <p>Example Field Design</p> <p> </p> <ul> <li>After uploading, verify that the preview of your data looks correct. If your file has column headers, leave the <code>Data has headers</code> box checked. Click <code>Choose columns</code>.</li> </ul> <p></p> <ul> <li>Drag and drop the columns into the appropriate headers used by the app. Navigate through the columns using the left / right arrows. Click <code>Import</code> once all required fields are filled. If columns have titles that exactly match those used by the app, the app's field will be filled automatically.</li> </ul> <p></p>"},{"location":"1.%20App/2-%20Amiga/#population-boundary","title":"Population Boundary","text":"<ul> <li>After importing data, select the date of the data processing will be done on from the dropdown. Select the version of the stitch you would like to use if multiple exist.</li> <li>Choose the <code>Draw</code> option on the right of the screen. Click along the outer perimeter of the portion of the orthomosaic you'd like to process. Use the <code>Edit</code>, <code>Translate</code>, and <code>Select</code> options to modify the boundary as needed. When finished, click <code>Save</code>.</li> <li>Click <code>Proceed</code> to continue to the Plot Boundary.</li> </ul>"},{"location":"1.%20App/2-%20Amiga/#plot-boundary","title":"Plot Boundary","text":"<ul> <li>NOTE: ALL PLOTS with plot boundaries created must be included in Field Design file even if the plots aren\u2019t being used in the field. Without their inclusion, mapping of accession information will be incorrect.</li> <li>Select the stitch once more from the dropdown menu. Click the icon at the bottom left to populate parameters for the plot boundaries.</li> <li>Adjust the parameters until you are satisfied with the plot boundaries for trait extraction. </li> <li>Use the options in the top right to modify the placement of individual plot boundaries, or all at once using <code>Select All</code>. The <code>Tooltips</code> option can be toggled to show plot / accession information on each plot to ensure the boundaries are properly placed.</li> <li>NOTE: For Amiga data processing, these boundaries do not need to be placed to exactly align with each edge of the stitch. The key is to make sure each boundary is clearly associated with one of the visible stitches (not covering multiple plots). </li> <li>When finished, click <code>Save</code>. </li> </ul>"},{"location":"1.%20App/2-%20Amiga/#assign-plot-labels","title":"Assign Plot Labels","text":"<ul> <li>If an AgRowStitch option is selected, this step should automatically appear.</li> <li>After defining the plot boundaries, select the date you have used for plot boundary prep in the dropdown.</li> <li>Press <code>Associate Plots with Boundaries</code> to save the associations of marked plot indexes with the plot labels and accessions.</li> <li>This association will only need to be done once per experiment and associations will be kept for future dates. If a new upload includes additional plots versus the reference date, the plot boundaries can be modified to associate the new plots as needed. </li> </ul>"},{"location":"1.%20App/2-%20Amiga/#processing","title":"Processing","text":""},{"location":"1.%20App/2-%20Amiga/#training-and-labeling","title":"Training and Labeling","text":"<ul> <li>Currently, Roboflow is recommended for labeling of images and model training.</li> <li>Plot images for model training can be downloaded from the Manage Files tab in the data table. <ul> <li>If plots have been marked / filtered for the date the download is requested for, only images associated with plots will be downloaded to avoid downloading images that are unlikely to contain useful training data.</li> </ul> </li> </ul>"},{"location":"1.%20App/2-%20Amiga/#predict","title":"Predict","text":"<ul> <li>Run inference on the cloud or locally</li> <li>Cloud inference requires roboflow credits</li> <li>Local inference requires a basic plan or above</li> <li>Insert your API Key and Model ID</li> <li>Select your Date, Platform, Sensor and Orthomosaic to run inference on</li> <li>After inference is completed, results can be viewed per plot with a confidence slider available <ul> <li>Currently only object detection is supported </li> </ul> </li> </ul>"},{"location":"1.%20App/2-%20Amiga/#trait-viewing","title":"Trait Viewing","text":"<p>To navigate to the statistics page, click on the <code>Stats</code> Icon. Use the icon with three lines to open and close the data selection menu.</p> <p></p> <ul> <li>The stats page shows processed data in tabular and graphical formats.</li> <li>To see statistics from a given date, expand the dropdown menu of the proper platform and sensor type. </li> <li> <p>Click Load on the date's table or graph column to view the extracted trait data.</p> </li> <li> <p>For the tabular format, you will see a window showing the table of traits. If desired, Download CSV can be clicked for the table as a CSV file.</p> </li> </ul> <p></p> <ul> <li>For the graphical format, you will see a window showing a bar graph representing the number of plots that fell within certain ranges for the currently selected trait.</li> <li>To view the graph of a different trait, change the selected trait at the top of the window.</li> <li>To view the distribution of a certain accession value (as populated in the uploaded field design file), use the dropdown menu to select a different accession. To view the distribution of all accessions, leave the accession as None.</li> <li>To save the current graph view as an image, click Save As Image in the bottom left corner.</li> </ul> <p></p> <p>To navigate to the map page, click on the <code>Map</code> Icon. Use the icon with three lines to open and close the data selection menu.</p> <ul> <li>Populate the data selection menu with the <code>Year</code>, <code>Experiment</code>, <code>Location</code>, <code>Population</code>, <code>Date</code>, <code>Platform</code>, and <code>Sensor</code> you wish to view.</li> <li>The map will automatically adjust to show the full orthomosaic generated for the parameters selected. </li> <li>Use the dropdown menu to select the <code>Trait Metric</code> to view. After selecting, use the next dropdown menu to select the <code>Genotype</code> to view.</li> <li>The plot boundaries formed in the population boundary step will show a range of colors based on the trait selected. Use the key at the bottom of the map to interpret the colors shown.</li> </ul>"},{"location":"1.%20App/2-%20Amiga/#query","title":"Query","text":"<p>To navigate to the query page, click on the <code>Query</code> Icon. Use the icon with three lines to open and close the data selection menu.</p> <ul> <li>After selecting <code>Year</code>, <code>Experiment</code>, <code>Location</code>, and <code>Population</code> in the data selection menu, populate the <code>Date</code>, <code>Platform</code>, and <code>Sensor</code> dropdowns to select the data to query.</li> <li>There are three options for image query available, with only one being used at a time:<ul> <li>Plot Numbers: Select from the dropdown menu the plot numbers whose images are to be queried. </li> <li>Accessions: Select the unique accession IDs of the images to be queried.</li> <li>Row / Column Pairs: Input Row / Column pairs (R, C) to select the images to be queried.</li> </ul> </li> <li>Click <code>View Images</code> to see the selected images to the right of the dropdown menus. Click <code>Download</code> to download the selected images as a .zip file.</li> <li>Images shown on the page can be hovered over to see the unique accession ID and plot number of each.</li> </ul> <p></p>"},{"location":"1.%20App/3-%20Drone/","title":"Drone Data Processing","text":""},{"location":"1.%20App/3-%20Drone/#upload-data","title":"Upload Data","text":"<p>To navigate to the file upload page, click on the <code>Upload</code> Icon.</p>"},{"location":"1.%20App/3-%20Drone/#data-types","title":"Data Types","text":"<p>Image Data: This is the raw image files taken from the sensing platforms. These group of images should be coming from an individual sensor. Some image types include: <code>jpg</code>, <code>jpeg</code>, <code>png</code> or <code>tif</code>. If your images do not contain EXIF metadata, you will need to upload a separate file called <code>msgs_synced.csv</code> under the data type Platform Logs.</p> <p>GCP Locations: This is a <code>csv</code> file containing locations of GCPs. The first column should contain the label, the second column contains latitude and the third column contains longitude.</p> <p>Example GCP Locations</p> <p></p> <p>Platform Logs: </p> <p>If you are missing EXIF metadata in your images, you need to upload a separate file and name it <code>msgs_synced.csv</code> under this datatype. This file should contain these columns:</p> <ul> <li><code>image_path</code>: This contains the name of the image (image_0000.jpg). The full path will be built after uploading the final file.</li> <li><code>time</code>: Date and time when the image is taken (YYYY:MM:DD HH:MM:SS)</li> <li><code>lat</code>: Latitude in decimal degrees</li> <li><code>lon</code>: Longitude in decimal degrees</li> <li><code>alt</code>: Altitude in meters</li> </ul> <p>Example msgs_synced.csv file</p> <p></p> <p>You can also upload drone logs into this datatype. You can check the Drone Operation Manual section for more information.</p>"},{"location":"1.%20App/3-%20Drone/#data-fields","title":"Data Fields","text":"<p>Year: This is the year <code>YYYY</code> the data was collected. It is a required field. For example, <code>2022</code> is a valid year.</p> <p>Experiment: This is open for the user to decide. This will differentiate experiments for a single year. For example, <code>Experiment 1</code> is a valid experiment.</p> <p>Location: This is the location where the data was collected. For example, <code>Davis</code> is a valid location.</p> <p>Population: This is the type of plants you will be analyzing. For example, <code>Legumes</code> is a valid population.</p> <p>Date: This is the date <code>YYYY-MM-DD</code> the data was collected. It is a required field. For example, <code>2022-06-20</code> is a valid date.</p> <p>Platform: This is the sensing platform used to collect the data. For example, <code>Drone</code> is a valid platform.</p> <p>Sensor: This is the data type collected. For example, <code>RGB</code> or <code>Thermal</code> are valid sensors.</p>"},{"location":"1.%20App/3-%20Drone/#upload-files","title":"Upload Files","text":"<ol> <li>Choose the <code>Data Type</code> you want to upload.</li> <li>Fill in the respective fields.</li> <li>Drag and drop your files in the upload region.  </li> </ol>"},{"location":"1.%20App/3-%20Drone/#video-demonstration","title":"Video Demonstration","text":"<p>File Upload Video</p>"},{"location":"1.%20App/3-%20Drone/#manage-files","title":"Manage Files","text":""},{"location":"1.%20App/3-%20Drone/#data-table","title":"Data Table","text":"<ul> <li>Navigate to the <code>Manage Files</code> tab to view uploaded data and prepare for processing. </li> <li>Hover over any of the action buttons in the <code>Actions</code> column to see their function.</li> </ul>"},{"location":"1.%20App/3-%20Drone/#orthomosaic-generation","title":"Orthomosaic Generation","text":"<p>To navigate to the image processing page, click on the <code>Process</code> Icon. Use the icon with three lines to open and close the data selection menu.</p>"},{"location":"1.%20App/3-%20Drone/#opendronemap","title":"OpenDroneMap","text":"<ul> <li>NOTE: To maximize orthomosaic output resolution, upload a gcp_locations.csv file in the <code>Upload</code> tab.</li> <li>You can also upload the GCP file right after selecting <code>Start</code> in the dropdown.</li> <li>Under the <code>Orthomosaic Method</code> dropdown, select OpenDroneMap. This is recommended for Aerial data with vertical and horizontal overlap.</li> <li>After uploading image files to the app, orthomosaic generation can be performed. </li> <li>Expand the dropdown menu of the correct platform and sensor type to select the date to perform generation on. </li> <li>Click the <code>Start</code> button to open the orthomosaic generation window. Use the <code>Previous</code> and <code>Next</code> buttons or the selection bar to iterate through images to mark all visible ground control points (GCPs) if a GCP Locations file was uploaded. If a GCP is placed in error, points can be removed by right-clicking.</li> <li>When ready to proceed, click <code>Generate Orthophoto</code>. In the settings dropdown, select the orthophoto quality. The <code>Custom</code> option can be used with OpenDroneMap args if more specific settings are needed. The default setting attempts to use a GSD (Ground Sampling Distance) of 0.01 cm/pixel. Use the Custom option to input specific GSD values if desired.</li> <li>After selecting the quality level, click <code>Process Images</code>. </li> <li>Follow the progress of orthomosaic generation by observing the progress bar at the bottom of the page. Progress can also be monitored via the logs, which can be opened using the arrow on the right side of the progress bar.</li> <li>Processing may take up to 2 hours with large image datasets. To decrease processing time, use the <code>Custom</code> setting with an increased <code>orthophoto-resolution</code> flag. For example, the <code>Custom</code> setting with <code>--orthophoto-resolution 1.0</code> would decrease processing time by a large amount. </li> <li>If any issues are seen with the output orthophoto using the viewer in the <code>Manage Orthomosaics</code> window, check the <code>GEMINI-App-Data/temp/project/code/odm_report/report.pdf</code> file for more details on the issues / GCP errors encountered during processing. </li> </ul>"},{"location":"1.%20App/3-%20Drone/#plot-boundary-preparation","title":"Plot Boundary Preparation","text":"<p>Import Data</p> <ul> <li>After ortho generation is finished, the plot boundary is prepared before further processing.</li> <li>First, input a field_design.csv file in the requested format:</li> </ul> <p></p> <p>Example Field Design</p> <p> </p> <ul> <li>After uploading, verify that the preview of your data looks correct. If your file has column headers, leave the <code>Data has headers</code> box checked. Click <code>Choose columns</code>.</li> </ul> <p></p> <ul> <li>Drag and drop the columns into the appropriate headers used by the app. Navigate through the columns using the left / right arrows. Click <code>Import</code> once all required fields are filled. If columns have titles that exactly match those used by the app, the app's field will be filled automatically.</li> </ul> <p></p> <p>Population Boundary</p> <ul> <li>After importing data, select the date of the data processing will be done on from the dropdown. Navigate to the location of the orthomosaic in the map.  </li> <li>Choose the <code>Draw</code> option on the right of the screen. Click along the outer perimeter of the portion of the orthomosaic you'd like to process. Use the <code>Edit</code>, <code>Translate</code>, and <code>Select</code> options to modify the boundary as needed. When finished, click <code>Save</code>.</li> <li>Click <code>Proceed</code> to continue to the Plot Boundary.</li> </ul> <p></p> <p>Plot Boundary</p> <ul> <li>NOTE: ALL PLOTS with plot boundaries created must be included in Field Design file even if the plots aren\u2019t being used in the field. Without their inclusion, mapping of accession information will be incorrect.</li> <li>Select the orthomosaic once more from the dropdown menu. Click the icon at the bottom left to populate parameters for the plot boundaries.</li> <li>Adjust the parameters until you are satisfied with the plot boundaries for trait extraction. </li> <li>Use the options in the top right to modify the placement of individual rectangles, or all at once using <code>Select All</code>. </li> <li>When finished, click <code>Save</code>. </li> </ul>"},{"location":"1.%20App/3-%20Drone/#trait-extraction","title":"Trait Extraction","text":"<ul> <li>After preparing the plot boundary, aerial traits can be processed.</li> <li>To process traits, click <code>Start</code> on the appropriate date's traits column. Click <code>Analyze</code> to begin processing.</li> <li>If traits need to be processed more than once, click on the blue checkbox and click <code>Analyze</code> again.</li> <li>In the future, the Teach Traits tab will allow for use of trainable aerial models.</li> </ul>"},{"location":"1.%20App/3-%20Drone/#trait-viewing","title":"Trait Viewing","text":"<p>To navigate to the statistics page, click on the <code>Stats</code> Icon. Use the icon with three lines to open and close the data selection menu.</p> <p></p> <ul> <li>The stats page shows processed data in tabular and graphical formats.</li> <li>To see statistics from a given date, expand the dropdown menu of the proper platform and sensor type. </li> <li> <p>Click Load on the date's table or graph column to view the extracted trait data.</p> </li> <li> <p>For the tabular format, you will see a window showing the table of traits. If desired, Download CSV can be clicked for the table as a CSV file.</p> </li> </ul> <p></p> <ul> <li>For the graphical format, you will see a window showing a bar graph representing the number of plots that fell within certain ranges for the currently selected trait.</li> <li>To view the graph of a different trait, change the selected trait at the top of the window.</li> <li>To view the distribution of a certain accession value (as populated in the uploaded field design file), use the dropdown menu to select a different accession. To view the distribution of all accessions, leave the accession as None.</li> <li>To save the current graph view as an image, click Save As Image in the bottom left corner.</li> </ul> <p></p> <p>To navigate to the map page, click on the <code>Map</code> Icon. Use the icon with three lines to open and close the data selection menu.</p> <ul> <li>Populate the data selection menu with the <code>Year</code>, <code>Experiment</code>, <code>Location</code>, <code>Population</code>, <code>Date</code>, <code>Platform</code>, and <code>Sensor</code> you wish to view.</li> <li>The map will automatically adjust to show the full orthomosaic generated for the parameters selected. </li> <li>Use the dropdown menu to select the <code>Trait Metric</code> to view. After selecting, use the next dropdown menu to select the <code>Genotype</code> to view.</li> <li>The rectangles formed in the population boundary step will show a range of colors based on the trait selected. Use the key at the bottom of the map to interpret the colors shown.</li> </ul> <p> </p>"},{"location":"1.%20App/3-%20Drone/#query","title":"Query","text":"<p>To navigate to the query page, click on the <code>Query</code> Icon. Use the icon with three lines to open and close the data selection menu.</p> <ul> <li>After selecting <code>Year</code>, <code>Experiment</code>, <code>Location</code>, and <code>Population</code> in the data selection menu, populate the <code>Date</code>, <code>Platform</code>, and <code>Sensor</code> dropdowns to select the data to query.</li> <li>There are three options for image query available, with only one being used at a time:<ul> <li>Plot Numbers: Select from the dropdown menu the plot numbers whose images are to be queried. </li> <li>Accessions: Select the unique accession IDs of the images to be queried.</li> <li>Row / Column Pairs: Input Row / Column pairs (R, C) to select the images to be queried.</li> </ul> </li> <li>Click <code>View Images</code> to see the selected images to the right of the dropdown menus. Click <code>Download</code> to download the selected images as a .zip file.</li> <li>Images shown on the page can be hovered over to see the unique accession ID and plot number of each.</li> </ul> <p></p>"},{"location":"1.%20App/4-%20Roboflow/","title":"Roboflow Usage","text":"<p>To extract traits from uploaded images, the GEMINI App supports both use of models trained using Roboflow or locally.</p>"},{"location":"1.%20App/4-%20Roboflow/#getting-started-with-roboflow","title":"Getting Started with Roboflow","text":"<p>When using Roboflow, we recommend the following steps to get started:</p> <p>\u2022 Create a Roboflow account: app.roboflow.com</p> <p>\u2022 Create a new workspace or join a shared workspace.</p> <p>\u2022 Create a new project and name it as desired based on the trait(s) you are labeling. </p> <p>\u2022 Place this project in the correct location based on your workspace's standards for the images being uploaded (location / platform / sensor).</p>"},{"location":"1.%20App/4-%20Roboflow/#extracting-traits-using-roboflow","title":"Extracting Traits Using Roboflow","text":"<p>Traits can be extracted using Roboflow models through two possible methods:</p> <p>Method 1: Label and Train Custom Model</p> <p>\u2022 Download plot images from the manage files tab (ground-based) or plot image extractor (UAV-based). Upload them to your Roboflow project.</p> <p>\u2022 Use the Roboflow labeling tool to annotate your images.</p> <p>\u2022 Train your model in Roboflow.</p> <p>\u2022 Note your model ID, version, and API key.</p> <p>\u2022 Enter these attributes into the \"Predict\" section of the app to get results.</p> <p>Method 2: Use Pretrained Model</p> <p>\u2022 Select a pretrained model from Roboflow Universe that is close to your use case: universe.roboflow.com.</p> <p>\u2022 Use the Fork Project button to create a copy of the model in your workspace.</p> <p>\u2022 Note the model ID, version, and API key.</p> <p>\u2022 Enter these attributes into the \"Predict\" section of the app to get results.</p> <p>On the \"Predict\" step, note the difference between cloud (hosted) inference and local inference for Roboflow models. The local option is preferred to minimize cost on Roboflow, but the cloud option may be chosen for less powerful workstations.</p>"},{"location":"1.%20App/4-%20Roboflow/#using-non-roboflow-models","title":"Using Non-Roboflow Models","text":"<p>If desired, Roboflow can be avoided for trait extraction.</p> <p>\u2022 Train a YOLOv8 or YOLOv11 detection or segmentation model locally or on another service using labeled images.</p> <p>\u2022 Alternatively, you can pull the model weights from another pretrained model and upload those to the app.</p> <p>\u2022 Upload your model weights using the \"Select\" section of the app.</p> <p>\u2022 Select the local model upload option on the \"Predict\" step and choose the uploaded model.</p>"},{"location":"1.%20App/5-%20Labeling%20Guide/","title":"Labeling Guide","text":"<p>Comprehensive guides for labeling methodology for single class cowpea flowers and multiclass cowpea pods are shown below. More crop and trait specific guides will be added over time to standardize labels.</p>"},{"location":"1.%20App/5-%20Labeling%20Guide/#cowpea-flower-labeling","title":"Cowpea Flower Labeling","text":""},{"location":"1.%20App/5-%20Labeling%20Guide/#cowpea-pod-labeling","title":"Cowpea Pod Labeling","text":""},{"location":"1.%20App/Aerial%20Usage/","title":"Examples","text":""},{"location":"1.%20App/Aerial%20Usage/#example-data","title":"Example Data","text":"<p>Example data can be found in this link: Example Data</p> <p>Data includes:</p> <ul> <li>Drone Images (for Aerial-Based Trait Extraction)</li> <li>Rover Images (for Ground-Based Trait Extraction)</li> </ul>"},{"location":"1.%20App/Aerial%20Usage/#aerial-based-trait-extraction","title":"Aerial-Based Trait Extraction","text":"<p>Start Docker Desktop and open the GEMINI app. If at any point more specific instructions are needed, navigate to the documentation page for the tab in use. </p>"},{"location":"1.%20App/Aerial%20Usage/#upload","title":"Upload","text":"<ul> <li>Navigate to the Upload tab. If not already chosen, select Image Data in the <code>Data Type</code> field.</li> <li>Populate the following fields with information on the data to be uploaded. </li> <li>After all fields have been populated, drag and drop files into the upload box or click in the box to select files via the file explorer.</li> <li>Once you have selected all files from the dataset, click Upload to upload the images to the app.</li> <li>After the uploading process is finished, click Done.</li> <li>If you wish to upload GCP Locations, change the <code>Data Type</code> field to GCP Locations and follow the same upload process as before.</li> </ul>"},{"location":"1.%20App/Aerial%20Usage/#process","title":"Process","text":"<ul> <li>Navigate to the Process tab. In the data selection menu, select the <code>Year</code>, <code>Experiment</code>, <code>Location</code>, and <code>Population</code> of the previously uploaded data to be processed.</li> <li>Click Begin Data Preparation. In the Orthomosaic Generation window, expand the dropdown of the chosen <code>Platform</code> and <code>Sensor</code>. Click <code>Start</code> on the date to perform orthomosaic generation with.</li> <li>If GCP Locations were uploaded, use the slider bar and the Previous and Next buttons to step through images and mark the visible GCPs.</li> <li>Click Generate Orthophoto. Select the desired quality in the <code>Settings</code> dropdown. </li> <li> <p>Click Process Images to begin generation.</p> </li> <li> <p>Once generation has completed, click Done on the progress bar. Navigate to the Plot Boundary Preparation window. </p> </li> <li>Follow the instructions on importing of field design data. Once data is uploaded, proceed to the Population Boundary step.</li> <li>Select the orthomosaic to create the boundary for via the <code>Select an orthomosaic</code> dropdown.</li> <li>Use the Draw tool to outline the boundary of the portion of the orthomosaic you wish to analyze. </li> <li>Use the Edit, Select, and Translate tools to modify a created boundary.</li> <li>Click Save and Proceed when finished to proceed to the Plot Boundary step.</li> <li>Select the correct orthomosaic as before. Use the rectangle generation tool in the bottom left to create the number and size of rectangles needed for the scale of analysis needed.</li> <li>Use the available tools to edit the placement of the rectangles.</li> <li> <p>Click Save when finished.</p> </li> <li> <p>After creating the plot boundary, navigate to the Aerial Processing window.</p> </li> <li>Expand the dropdown of the chosen <code>Platform</code> and <code>Sensor</code> once more. Click Start to process the traits of the desired date. </li> </ul>"},{"location":"1.%20App/Aerial%20Usage/#stats","title":"Stats","text":"<ul> <li>Navigate to the Stats tab. Expand the dropdown of the chosen <code>Platform</code> and <code>Sensor</code> to see data for the available dates.</li> <li>Click Load to view the processed data of choice. Click Download CSV if desired.</li> </ul>"},{"location":"1.%20App/Aerial%20Usage/#map","title":"Map","text":"<ul> <li>Navigate to the Map tab. Open the data selection menu to select the data to analyze.</li> <li>Use the dropdown menu to select the <code>Trait Metric</code> to view.</li> <li>The traits can be seen overlaid on the orthomosaic based on the plot boundaries created earlier.</li> </ul>"},{"location":"2.%20Hardware/Amiga/1-%20AmigaQSG/","title":"Quick Start Guide","text":""},{"location":"2.%20Hardware/Amiga/1-%20AmigaQSG/#getting-started-with-the-farm-ng-amiga","title":"Getting Started with the farm-ng Amiga","text":"<ol> <li>Control the Amiga.<ul> <li>Go through the dashboard overview. We recommend getting  comfortable with the speed adjustment and e-stop configuration.</li> <li>We recommend going 100 - 200 ft/min (the first 2 major ticks) to  get the best image quality. See the farm-ng Amiga Dashboard Documentation for more information on use of the dashboard.  </li> </ul> </li> <li>Set up the GPS RTK base station.<ul> <li>Connect the powerbrick to the CAN/POWER input on the back of  the Brain. It will start doing its serve and pinning its position. </li> <li>The base station will get some time to have a good accuracy. Marcelo  recommends you power it up, use the UI to connect to WIFI, and  leave static for at least 20 min before connecting to receive  corrections.</li> <li>Once the base station is connected, you should see the GPS icon at  the top right of the Brain without the slash.</li> </ul> </li> <li> <p>Record some sample images in a field or with random objects.</p> <ul> <li>Follow the farm-ng Camera App Documentation on how to start and stop a recording. </li> <li>Have the GPS and the top camera (facing down) centered on the  Amiga.</li> <li>The side cameras can be placed as low as possible. Play around  with the angle of the side cameras and preview the images from  the Brain.</li> </ul> </li> <li> <p>Export the recorded file.</p> <ul> <li>You will need a USB drive plugged into the back of the brain. Make sure to only plug in the drive after the brain has been turned on.</li> <li>Open the file manager app to view   the images and export. Refer to the farm-ng File Manager App Documentation for more information on using the file manager for viewing and exporting.</li> </ul> </li> </ol>"},{"location":"2.%20Hardware/Amiga/1-%20AmigaQSG/#data-saving-and-export-sops","title":"Data Saving and Export SOPs","text":"<ul> <li>The GEMINI App will be used for extraction of the binary files recorded using the Amiga. Follow the app installation and setup instructions before proceeding.</li> <li>Once you are done recording, use the file manager app to export the binary files to an external drive.<ul> <li>Note that the USB drive must be ext4 or exFAT formatted. USB 3.0 drives are recommended for faster file-transfer.</li> <li>Once data has been successfully transferred to the drive, the files in the file manager can be deleted.</li> </ul> </li> <li>Using the upload tab of the GEMINI App, upload the binary files to begin extracting the binary files to viewable formats. <ul> <li>Viewing of the extracted data will be added soon to the GEMINI App.</li> </ul> </li> </ul>"},{"location":"2.%20Hardware/Amiga/2-%20Recording/","title":"Recording SOPs","text":""},{"location":"2.%20Hardware/Amiga/2-%20Recording/#pre-data-collection","title":"Pre-Data Collection","text":""},{"location":"2.%20Hardware/Amiga/2-%20Recording/#select-topics","title":"Select Topics","text":"<ul> <li>Before recording, ensure that the following topics are being recorded by going into your settings and going to the Recorder tab.</li> <li>To include the topics, press the Include URI button next to the topic name. </li> <li>The following topics should be selected for recording (create a profile if needed):</li> </ul> GPS Topics Oak0 Topics Oak1 Topics Oak2 Topics /gps/ecef oak/0/calibration oak/1/calibration oak/2/calibration /gps/pvt oak/0/disparity oak/1/disparity oak/2/disparity /gps/relposned oak/0/imu oak/1/imu oak/2/imu oak/0/rgb oak/1/rgb oak/2/rgb"},{"location":"2.%20Hardware/Amiga/2-%20Recording/#image-quality-assurance","title":"Image Quality Assurance","text":"<ul> <li>Take a sample recording while driving the Amiga to the field (at the speed you hope to drive the Amiga through the field). </li> <li>View the recording in the file manager app to observe any issues with blur, exposure, occlusion, shadows, or camera position / perspective. <ul> <li>Use the dropdown in the top left to toggle which Oak camera's recording is shown.</li> </ul> </li> <li>Before beginning recording for the day, use the settings icon in the Camera App to show each camera's output on the display. <ul> <li>Use the Auto Exposure and Auto Focus buttons to recalibrate each camera if there appears to be an issue.</li> </ul> </li> <li>Once you have verified that the image quality looks correct, recording can begin once the GPS RTK connection has been made. </li> </ul>"},{"location":"2.%20Hardware/Amiga/2-%20Recording/#gps-quality-assurance","title":"GPS Quality Assurance","text":"<ul> <li>Connect GPS Base Station to WiFi and ensure that the attena is connected to the base station.</li> <li>It is recommended that the attena is placed outside with a clear view of the sky.</li> <li>Connect the Amiga to WiFi as well and go to the settings GPS NTRIP tab.</li> <li>NOTE: The Amiga and Base Station DO NOT need to be on the same WiFi network, but they do need to be connected to the internet.</li> <li>Ensure that a green notification appears in the top right corner of the screen indicating that the GPS is connected to the base station.</li> </ul>"},{"location":"2.%20Hardware/Amiga/2-%20Recording/#data-collection","title":"Data Collection","text":"<ul> <li>Please go through the Pre-Data Collection steps before proceding.</li> <li>We assume your field experiment is split into columns and rows. </li> <li>Drive your Amiga to the start of the first column and press the Record button in the Camera Recorder App.</li> <li>The Amiga will begin recording the camera data and GPS data.</li> <li>At the end of the first column, press the Stop button in the Camera Recorder App.</li> <li>If you have different experiments in the same column, you may stop recording at any point and start a new recording.</li> <li>However, it is difficult to track recordings if you stop and start recording multiple times in the same column.</li> </ul>"},{"location":"2.%20Hardware/Amiga/3-%20Autoplot/","title":"Autoplot","text":""},{"location":"2.%20Hardware/Amiga/3-%20Autoplot/#autoplot-prerequisites","title":"Autoplot Prerequisites","text":"<ul> <li>Open the <code>Settings</code> tab from the Brain home screen.</li> </ul>"},{"location":"2.%20Hardware/Amiga/3-%20Autoplot/#imu-calibration","title":"IMU Calibration","text":"<ul> <li>If you have not calibrated the rover IMU or have made recent changes to the rover, use the <code>IMU Calibration</code> settings pane to calibrate.</li> <li>With the rover on flat ground, click <code>Start Calibration</code> to begin.</li> <li>You will see a popup to remind you to move the rover to flat ground. Click <code>Confirm</code> to continue. Calibration should complete within a few seconds.</li> </ul>"},{"location":"2.%20Hardware/Amiga/3-%20Autoplot/#robot-geometry","title":"Robot Geometry","text":"<ul> <li>The autoplot app requires accurate robot geometry information to be entered in the brain.</li> <li>Measure the wheelbase and track width using a tape measure.</li> <li>Find the center point of the amiga using these measurements.</li> <li>Measure the placement of the top camera (OAK0) for IMU Offset and the GPS antenna for GPS Offset relative to this center point.</li> <li>The x and y offset definitions can be seen in the image below (via farm-ng). Facing the direction of the dashboard, negative y values are defined as right of the center point while negative x values are defined as behind the center point.  </li> </ul> <ul> <li>Once the offsets have been found, update the values in the <code>Robot Geometry</code> pane of the settings tab.</li> <li>Click <code>Apply</code> to save the robot geometry and proceed.</li> </ul>"},{"location":"2.%20Hardware/Amiga/3-%20Autoplot/#gps-ntrip-wifi","title":"GPS NTRIP / WiFi","text":"<ul> <li>Ensure base station credentials are populated in the <code>GPS NTRIP</code> settings pane.</li> <li>When GPS is properly connected, the GPS symbol will not be crossed out and the <code>GPS RTK CONNECTED</code> message will be present on the <code>GPS NTRIP</code> tab, as seen here.</li> <li>Make sure the amiga is connected to WiFi.</li> </ul>"},{"location":"2.%20Hardware/Amiga/3-%20Autoplot/#using-autoplot","title":"Using Autoplot","text":""},{"location":"2.%20Hardware/Amiga/3-%20Autoplot/#path-creation","title":"Path Creation","text":"<ul> <li>After satisfying the prerequisites, open the <code>Autoplot</code> app from the amiga Brain. </li> <li>After some load time, two camera streams will be visible on the left side of the screen.</li> <li>To create a new path, click the <code>New Path</code> button. Begin driving the amiga along the path using the pendant. </li> </ul> <ul> <li>When you've reached the end of the desired path, click <code>Save Path</code> and give the recorded path a name. If you deviate from the desired path, you can click <code>Pause</code> and <code>Remove Point</code> to fix the path before resuming recording.</li> </ul>"},{"location":"2.%20Hardware/Amiga/3-%20Autoplot/#path-following","title":"Path Following","text":"<ul> <li>To follow a path, click the <code>Load Path</code> button to see the saved path recordings. Select the path you'd like to load.</li> </ul> <ul> <li>Now, the dashboard has to be set to auto-control mode to start the path.</li> <li>On the dashboard, click the <code>A</code> symbol to open the auto control tab.</li> </ul> <ul> <li>In the auto control tab, press the <code>AUTO CONTROL</code> button to turn auto control on.</li> </ul> <ul> <li>Once auto control has been turned on and all other prerequisites have been met, the status indicators on the amiga Brain should be as shown:</li> </ul> <ul> <li>If there are no issues, the <code>Start Path</code> button can now be clicked in the Autoplot app. Path following will work best if you <code>Start Path</code> with the amiga close to the starting point facing the same direction as when the recording was started.</li> </ul>"},{"location":"2.%20Hardware/Amiga/3-%20Autoplot/#debugging","title":"Debugging","text":"<ul> <li>If any prerequisite service on the status bar is crossed out, click the icon for more information. </li> <li>If the <code>Start Path</code> button is still shows a question mark, press the button for information on the error. </li> <li>Errors related to GPS accuracy can be avoided by adjusting the thresholds in the <code>Robot Localization</code> settings pane.</li> <li>Guidelines on adjustment of the distance and time thresholds are provided within the settings pane. We recommend increasing the <code>Path Deviation Threshold</code> and <code>Minimum GPS Accuracy</code> values to avoid errors. The exact values to increase to should be determined by in-field testing and may vary based on distance from RTK base station.</li> <li>To adjust the speed of the amiga in auto control mode, open the <code>Track Following</code> settings pane and adjust values as needed. </li> <li>A maximum linear speed of 0.5 to 1 m/s is recommended to maximize image quality.</li> </ul>"},{"location":"2.%20Hardware/Amiga/4-%20Custom%20Amiga%20Apps/","title":"Custom Amiga Apps","text":""},{"location":"2.%20Hardware/Amiga/4-%20Custom%20Amiga%20Apps/#prerequisites","title":"Prerequisites","text":""},{"location":"2.%20Hardware/Amiga/4-%20Custom%20Amiga%20Apps/#device-access","title":"Device Access","text":"<ul> <li>The amiga brain must be connected to WiFi for any on-device app development.</li> <li>Follow the farm-ng documentation on requesting device SSH access and configuring SSH keys. </li> <li>Follow the Tailscale quickstart guide  using the same Google account used in your farm-ng fleet manager login for your tailnet login. </li> <li>Add the device you intend to develop from to your tailnet. Marcelo will share the link to add your amiga to your tailnet.</li> </ul>"},{"location":"2.%20Hardware/Amiga/4-%20Custom%20Amiga%20Apps/#development-environment","title":"Development Environment","text":""},{"location":"2.%20Hardware/Amiga/4-%20Custom%20Amiga%20Apps/#vscode","title":"VSCode","text":"<ul> <li>VSCode is recommended for app development on the amiga.</li> <li>With Tailscale active on your device (using <code>sudo tailscale up</code> on Linux or opening the app on Windows or Mac), find the amiga's IP-address.</li> <li>Follow the instructions from farm-ng for configuration of SSH connections using VSCode. </li> </ul>"},{"location":"2.%20Hardware/Amiga/4-%20Custom%20Amiga%20Apps/#ui-testing","title":"UI Testing","text":"<ul> <li>To develop apps without physical access to the brain, a method to view the UI is needed.</li> <li>After configuring and opening Tailscale, type the brain IP address into a browser. </li> <li>After some load time, the brain UI should be visible in the browser. </li> </ul>"},{"location":"2.%20Hardware/Amiga/4-%20Custom%20Amiga%20Apps/#app-development","title":"App Development","text":""},{"location":"2.%20Hardware/Amiga/4-%20Custom%20Amiga%20Apps/#farm-ng-app-templates","title":"farm-ng App Templates","text":"<ul> <li>farm-ng provides app templates that can be used to simplify app development together with app examples.</li> <li>Depending on your preference, you can use the ReactJS farm-ng app guide or Kivy farm-ng app guide to start developing.</li> <li> <p>Using Github, create a new personal repository from the desired app template: Kivy Template / ReactJS Template </p> </li> <li> <p>After clicking <code>Create a new repository</code>, name your new app repository and add a description as desired. Choose whether the repository should be public or private. Click <code>Create repository</code> when finished. </p> </li> </ul>"},{"location":"2.%20Hardware/Amiga/4-%20Custom%20Amiga%20Apps/#cloning-the-new-app","title":"Cloning the New App","text":"<ul> <li>The new app now needs to be cloned to the amiga brain for development. </li> <li>Open a new VSCode window. Open the source control tab from the left sidebar. </li> <li>Click <code>Clone repository</code>. Next, either choose <code>Clone from Github</code> or paste in the repository URL from the repository's <code>Code</code> button.</li> <li>If you have not previously set up Git credentials for cloning repositories, follow the Github docs on remote repositories and cloning methods.</li> </ul>"},{"location":"2.%20Hardware/Amiga/4-%20Custom%20Amiga%20Apps/#running-the-app","title":"Running the App","text":"<ul> <li>After cloning the repository and developing the app using the examples and information shown in the ReactJS farm-ng app guide or Kivy farm-ng app guide, the app can be tested.</li> <li>For apps using the UI, go to the amiga IP address in a browser to see and interact with the brain.</li> <li>In the VSCode terminal, locate the <code>entry.sh</code> script to be used to open the app.</li> <li>Make the script executable by entering <code>sudo chmod +x entry.sh</code> in the terminal.</li> <li>Run <code>./entry.sh</code>. The app or any error starting the app should be seen on the UI. Logging information on the app start will be shown in the terminal.</li> <li>Make sure to change the path saved in the <code>exec_cmd</code> field to the correct <code>entry.sh</code> location and the <code>display_name</code> to the app name in the <code>manifest.json</code> file to allow the app to properly launch from the UI and be displayed on the apps screen. </li> <li>When the app is finished and the ability to launch from the UI is desired, make the <code>install.sh</code> script executable as shown before and run the script to add the app icon to the apps screen.</li> </ul>"},{"location":"2.%20Hardware/Drone/1%20-%202024-11-18%20Assembling%20propellers/","title":"Building GEMINI Drone","text":""},{"location":"2.%20Hardware/Drone/1%20-%202024-11-18%20Assembling%20propellers/#propeller-assembly-instructions","title":"Propeller Assembly Instructions","text":"<p>The GEMINI quadcopter is based on Arducopter, and the motor layout follows their QUAD X configuration.</p> <p> </p> <p>Assemble the propellers by matching the motors' rotation direction with the propellers' rotation direction. The correct propeller rotation direction is the one that pushes air downwards when spinning. The correct orientation can be illustrated below when holding the propeller at eye level and looking at the blades.</p> <p></p> <p>Install the propellers onto the motor mounts using the included M3 screws. Tighten the screws firmly enough to prevent the propellers from detaching during flight. However, be careful not to overtighten, as this could damage the propeller and the motor threads.</p> <p> </p>"},{"location":"2.%20Hardware/Drone/2%20-%20iPhone%20add-on%20for%20DJI%20Phantom%204/","title":"iPhone add-on for DJI Phantom 4","text":""},{"location":"2.%20Hardware/Drone/2%20-%20iPhone%20add-on%20for%20DJI%20Phantom%204/#dji-phantom-4-for-gemini-sensing","title":"DJI Phantom 4 for GEMINI Sensing","text":"<p>A 3D-printable model has been developed to mount an iPhone under the DJI Phantom 4 for remote sensing. </p> <p></p> <p>Download STL files: Phantom4_iPhone13_Mount_Legs |  Phantom4_iPhone13_Mount_Tray</p> <p></p> <p>For printing, PETG filament is recommended over PLA due to its greater heat resistance. Follow the extruder temperature and the manufacturer's recommended temperature for the filament. After 3D printing, use a heat gun to bend the drone legs to the desired angle. Bending the legs provides better stability during landing and prevents them from appearing in the field of view when simultaneously capturing video with the DJI gimbal camera.</p> <p></p> <p>For PETG, set the heat gun temperature to 230 degrees Celsius.</p> <p></p> <p>When bending the legs outward, the outer side is more prone to tearing due to greater length deformation. Therefore, apply heat evenly to the outer side to make the bending area pliable. However, avoid excessive heat, as this can cause damage.</p> <p></p> <p>After applying heat, place the leg on a flat surface and bend it. If the angle is insufficient, apply additional heat to the bending area with the heat gun to achieve the desired angle.</p> <p></p> <p></p> <p></p> <p>Assemble the bent leg parts and the tray part using M3 15mm pan head screws. Then, insert the wire that secures the assembly to the drone legs.</p> <p></p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/","title":"Drone Operation Manual","text":""},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#drone-operation-manual","title":"Drone Operation Manual","text":""},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#installing-mission-planner","title":"Installing Mission Planner","text":"<p>You can follow the instructions from Installing Mission Planner \u2014 Mission Planner documentation</p> <p>For Android Tablets, you can download and install this APK file: </p> <ul> <li>Release Android Development Build \u00b7 ArduPilot/MissionPlanner \u00b7 GitHub</li> <li>com.michaeloborne.MissionPlanner-signed.apk</li> </ul>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#using-a-tablet-or-pc","title":"Using a Tablet or PC?","text":"<p>You can connect the drone to an android tablet using USB OTG, the same as a PC. Using a tablet, you can monitor the drone's status and telemetry data when you are out in the field. However, using a laptop PC is recommended if you want to change detailed settings.</p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#how-to-make-survey-missions-in-mission-planner","title":"How to Make Survey Missions in Mission Planner","text":"<ol> <li>Open the Mission Planner program.</li> </ol> <ol> <li>Click \"Plan\".</li> </ol> <ol> <li>The home location will be used as a starting point when you're making the flight missions. If you connect the drone, you can see where the home position is. If you can't see the maps of your field, you can manually set the home location by entering the Latitude and Longitude or clicking on the map.</li> </ol> <p>For example, the UC Davis RMI Vineyard location is:     <code>Latitude: 38.5323259361796    Longitude: -121.752081513405</code></p> <p>You can also set the default altitude for your waypoints by setting \"Default Alt\".</p> <p></p> <ol> <li>Draw a Polygon using the right-click menu \u2192 Polygon \u2192 Draw a Polygon.</li> </ol> <p></p> <ol> <li>To create a survey mission, right-click \u2192 Auto WP \u2192 Survey (Grid).</li> </ol> <p></p> <ol> <li>You can see the generated paths, but you have to modify the parameters. Click \"Advanced Options\". It will show more settings.</li> </ol> <p></p> <ol> <li>Select the Camera to estimate the camera's field of view. </li> </ol> <p></p> <ul> <li> <p>Choose FLIR Vue 336 6.8mm because the FLIR One Pro's FOV is 50x40 and FLIR Vue 336 6.8mm is 45x35.</p> <p></p> </li> <li> <p>Set the flight speed. Recommended speed: 2m/s ~ 4m/s.</p> </li> <li> <p>Check the total flight time and the period for Photo every.</p> <p></p> </li> <li> <p>Optimize the Polygon by only showing the Polygons and move the points.</p> <p></p> </li> <li> <p>Make sure the flight time is within the drone's max flight time (20 minutes).</p> </li> <li> <p>Check \"Use speed for this mission\", \"Add Takeoff and Land WPs\", and \"Use RTL\" if you want the drone to automatically land at the launch point after the mission.</p> <p></p> </li> <li> <p>Click \"Accept\".</p> </li> <li> <p>Save the polygon and waypoints to a file.</p> </li> <li> <p>Save polygon just in case it's needed again.</p> <p></p> </li> <li> <p>Save the waypoint file to reload the waypoints to the program</p> <p></p> </li> <li> <p>Write the waypoints to the drone when it's connected to the Mission Planner Software.</p> </li> </ul> <p></p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#how-to-prepare-the-drone-flight","title":"How to Prepare the Drone Flight","text":"<ol> <li>Open the box and bring out the drone.</li> </ol> <ol> <li>Mount the iPhone and FLIR One Pro to the tray.</li> </ol> <ol> <li>Expand the arm until it's clicked to the arm clip.</li> </ol> <ol> <li>Install the Li-Po battery and tie it.</li> </ol> <ol> <li>Power up the drone and plug the USB Radio module into your computer. Be aware of the plus and minus of the connector.</li> </ol> <ol> <li>Open the Mission Planner Application.</li> </ol> <ol> <li>Select AUTO and Click Connect. It will automatically scan the port and find the USB radio module.</li> </ol> <ol> <li>Now you can see the location and status of the drone.</li> </ol> <ol> <li>Press the hardware safety switch to activate the drone.</li> </ol> <ol> <li> <p>Arm the motors by pressing the left stick to the lower right corner and hold it.</p> <p>Note: GEMINI drone controller is MODE 2 to comply with most recent drone pilots. If you want to change the controller setting, please refer below:</p> <ul> <li>What is the difference between Mode 1 and Mode 2?</li> <li>Flysky FS-i6X Transmitter Mode 1 To Mode 2 Modification</li> <li>FlySky FSi6 - MODE 1 or MODE 2 Setup</li> </ul> <p></p> </li> <li> <p>Now you can fly the drone.</p> </li> <li> <p>To disarm the motors after the flight, press the left stick to the lower left corner and hold it.</p> <p></p> </li> </ol>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#how-to-start-the-auto-mission","title":"How to Start the Auto Mission","text":"<ol> <li> <p>Follow the instructions from \"How to prepare the drone flight\"</p> </li> <li> <p>Write the waypoints to the drone when it's connected to the Mission Planner Software.</p> </li> </ol> <p></p> <ol> <li>Press the hardware safety switch to activate the drone.</li> </ol> <p></p> <p></p> <ol> <li>Arm the motors by pressing the left stick to the lower right corner and hold it. </li> </ol> <p></p> <ol> <li>Lift the Throttle to 20~30% and Toggle the Auto switch. It will automatically launch and start the mission.</li> </ol> <p></p> <ol> <li>If you set auto landing, it will return to the launch position and automatically land. If you did not set the auto return, it will stay at the last waypoint in the Mission, and you will have to land the drone manually.</li> </ol>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#useful-videos","title":"Useful Videos","text":"<p>Introduction to Mission Planner - YouTube</p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#how-to-use-drone-battery-charger","title":"How to Use Drone Battery Charger","text":""},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#introduction","title":"Introduction","text":"<p>This document explains how to charge a drone battery using a drone battery charger. After reading this, you will know how to charge a drone battery, how to set the battery voltage to storage mode after a flight, precautions to take when charging the battery, and how to respond in case of errors.</p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#battery-specs","title":"Battery Specs","text":"<p>GEMINI Custom drones come with 7000 ~ 10000 mAh Lithium polymer batteries. The maximum possible flight time varies depending on the battery capacity and the drone's payload, but generally, about 25 minutes of flight is possible.</p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#charger-specs","title":"Charger Specs","text":"<p>For this project, we use a charger capable of balance charging lithium polymer batteries. Balance charging means equalizing the voltage of each cell when multiple cells, such as in a 6S Lipo battery, are connected. The battery connector used is an XT60 connector.</p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#setting-up-charger","title":"Setting Up Charger","text":"<p>Open the box and find the cables. The first cable is a banana plug to XT60 plug converter. Connect the two banana cables to the charger, paying attention to the red (+) and black (-) colors.</p> <p>Next, connect the balance port extension board. Connect the balance board extension cable from the box to the charger main unit, and then connect the cable to the extension board.</p> <p></p> <p>Plug the charger's main power cable into a wall outlet and turn on the power switch on the back of the main unit.</p> <p></p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#charger-setting","title":"Charger Setting","text":"<p>The charger consists of two independently operating units in one main body; Unit 1 controls the left port, and Unit 2 controls the right port.</p> <p></p> <p>When you first start the charger, the PROGRAM SELECT screen will appear. Use the (+) and (-) buttons to select the Lithium BATT screen, then press the \u25b6\ufe0f button to select it. The LiPo Balance CHG screen will then appear.</p> <p></p> <p>Next, connect the battery. After connecting the battery, check that the voltage setting is at 22.2V (6S). If the voltage setting is incorrectly set to 18.5 (5S) or similar, press the \u25b6\ufe0f button to make it blink, then set it to 22.2V (6S).</p> <p></p> <p>Next, press and hold the \u25b6\ufe0f button to start charging. The charger will check if the connected battery's voltage matches the charging specs, and if not, an error will appear.</p> <p></p> <p>If everything is normal, press the \u25b6\ufe0f button once more to start charging. Wait about 2 hours for a full charge.</p> <p></p> <p>While charging, you can press the (+) button to check the voltage of each cell. If the voltage of a particular cell is too low or too high, it is likely that there is a problem with the battery, so it is recommended to stop charging the battery.</p> <p></p> <p>By pressing the (\u2013) button, you can view the charging parameters. Here, you can check whether the final voltage is set appropriately, as well as how the charge capacity cut-off and charge time cut-off are configured. If you wish to change these settings, please refer to the manufacturer's user manual.</p> <p></p> <p></p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#battery-storage-mode","title":"Battery Storage Mode","text":"<p>Lithium polymer batteries used in drones should not be stored for more than a week in a fully discharged or fully charged state, as this can shorten battery lifespan and cause malfunctions. Therefore, when storing drone batteries for an extended period without use, it is recommended to charge or discharge each cell to the storage voltage of 3.8V. </p> <p>On the charger, press the \u25b6\ufe0f button to make the charger mode setting blink, then press the (-) or (+) button to switch from Balance CHG to Storage mode. Next, press and hold the \u25b6\ufe0f button to enter Storage mode.</p> <p></p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#why-are-drone-flight-logs-needed","title":"Why Are Drone Flight Logs Needed?","text":"<p>Images collected from the drone contain the camera's GPS information in the EXIF data. However, to use more accurate GPS information and altitude data, you can use the drone's Flight log. Drone altitude information is measured using a down-facing LiDAR, which is useful for image alignment and predicting GSD. The GEMINI-App utilizes drone logs to synchronize the timestamp of images captured by the drone with the GPS timestamps in the log, replacing the image's original GPS information with the drone's GPS coordinates and altitude data.</p>"},{"location":"2.%20Hardware/Drone/3%20-%20Drone%20SOP/#how-to-download-drone-flight-log","title":"How to Download Drone Flight Log","text":"<p>After the flight, you can connect Mission Planner to download the log.</p> <ol> <li>Connect the drone to the computer using a USB cable.</li> </ol> <p></p> <ol> <li>Launch the Mission Planner app.</li> </ol> <p></p> <ol> <li>Press the CONNECT button to connect to the drone.</li> </ol> <p></p> <ol> <li>Select the \"DataFlash Logs\" tab and press the \"Download DataFlash Log Via Mavlink\" button.</li> </ol> <p></p> <ol> <li>When the Log Downloader window appears, (1) check the logs to download and (2) select Download Selected Logs to download. Check the right-side \"Output:\" to confirm that the log download is complete.</li> </ol> <p></p> <ol> <li>The downloaded log can be found at C:\\Users\\PAIBL\\Documents\\Mission Planner\\logs\\QUADROTOR\\1.</li> </ol> <p></p> <ol> <li>Upload the downloaded log using the GEMINI-App. Select the logs with the extension *.bin that correspond to the date of the drone images to upload.</li> </ol> <p></p> <p></p> <p></p> <p></p>"},{"location":"2.%20Hardware/Drone/4%20-%20Ground%20Control%20Points%20%28GCPs%29/","title":"Ground Control Points (GCPs)","text":""},{"location":"2.%20Hardware/Drone/4%20-%20Ground%20Control%20Points%20%28GCPs%29/#introduction-to-ground-control-points-gcps","title":"Introduction to Ground Control Points (GCPs)","text":"<ul> <li> <p>Definition and purpose of GCPs:     Ground Control Points (GCPs) are points on the ground with known latitude, longitude, and altitude used to georeference remote sensing images. This involves placing markers on the ground and measuring their positions using GPS or other methods. The size of the GCPs can vary depending on the resolution of the remote-sensing imagery.</p> </li> <li> <p>Importance of GCPs in drone surveying and mapping:     GCPs' positional accuracy affects the positional accuracy of remote sensing images. They also provide ground truth key points for drone image processing.</p> <p>For more information, refer to;</p> <p>Ground control points: why are they important? | Pix4D</p> <p>How Do I Use Ground Control Points?</p> </li> </ul>"},{"location":"2.%20Hardware/Drone/4%20-%20Ground%20Control%20Points%20%28GCPs%29/#types-of-ground-control-points","title":"Types of Ground Control Points","text":"<ul> <li> <p>Common types of GCPs used in drone remote sensing:     Remote sensing often utilizes identifiable fixed objects with known latitude and longitude values obtainable from sources like Google Maps or other satellite images. However, agricultural fields sometimes need dedicated GCPs due to the need for identifiable and stationary objects. In drone remote sensing, GCPs (Ground Control Points) are either installed in the field as portable tarp-like GCPs for each drone flight or they are left in place throughout the remote sensing project. GCPs are typically designed as checkerboard patterns for easy identification in digital images and are placed in the field.</p> </li> <li> <p>Examples of GCPs     Example of GCP near Golden Gate Bridge </p> <p>Example of a GCP plate in the field </p> </li> </ul>"},{"location":"2.%20Hardware/Drone/4%20-%20Ground%20Control%20Points%20%28GCPs%29/#setting-up-ground-control-points","title":"Setting Up Ground Control Points","text":"<ul> <li> <p>GCPs for Low Altitude Drone Remote Sensing For low-altitude, high-resolution drone remote sensing, GCPs (Ground Control Points) the size of a Letter or A4 paper are sufficient for identification. For example, when acquiring imagery at an altitude of 10 meters using a DJI Phantom 4, a resolution of 0.27cm/pixel can be achieved. This means a GCP made with Letter-size paper would appear approximately 80x100 pixels. </p> </li> <li> <p>Example GCP file used in GEMINI Project The GCPs are designed with a checkered pattern for easy identification in aerial imagery. To prevent confusion when numbers are flipped (e.g., 6 and 9), an underline has been added to each number. PDF files for GCPs numbered 1 through 16 are attached, along with Microsoft PowerPoint format files.  Download GCP PDF |  Download GCP PowerPoint</p> </li> </ul>"},{"location":"2.%20Hardware/Drone/4%20-%20Ground%20Control%20Points%20%28GCPs%29/#gcp-location-selection-criteria","title":"GCP location selection criteria","text":"<ul> <li> <p>Total Number of GCPs     For a typical drone remote sensing project, it is advisable to have 5 to 10 GCPs (Ground Control Points) evenly distributed across the field to be surveyed.</p> </li> <li> <p>Choosing optimal locations (e.g., corners of buildings)     When utilizing existing terrain features as GCPs, it is recommended to select points where the color of the terrain changes or corners as GCPs.</p> </li> <li> <p>Avoiding obstructions and shadows     If there are objects taller than the GCPs in the vicinity, shadows may be cast on the GCPs or they may be obscured in images depending on the relative positions of the drone and the GCPs. Therefore, such locations should be avoided, or obstacles should be removed.</p> </li> </ul>"},{"location":"2.%20Hardware/Drone/4%20-%20Ground%20Control%20Points%20%28GCPs%29/#references","title":"References","text":"<p>Ground control points: why are they important? | Pix4D How Do I Use Ground Control Points? A Comprehensive Guide to Using Ground Control Points for Drone Surveying - JOUAV Ground Control Points | DJI Enterprise</p>"},{"location":"2.%20Hardware/Drone/4%20-%20Ground%20Control%20Points%20%28GCPs%29/#measuring-gcps","title":"Measuring GCPs","text":""},{"location":"2.%20Hardware/Weather%20Station/1-%20Weather%20Station%20Build/","title":"Build Guide","text":""},{"location":"2.%20Hardware/Weather%20Station/1-%20Weather%20Station%20Build/#gemini-weather-station-build-guide","title":"GEMINI Weather Station Build Guide","text":""},{"location":"2.%20Hardware/Weather%20Station/1-%20Weather%20Station%20Build/#component-list","title":"Component List","text":"<ul> <li>Arduino Uno R3 (or equivalent): Documentation</li> <li>Datalogger Shield: Documentation</li> <li>DHT22 Temperature / Humidity Sensor: Documentation</li> <li>HM-10 Bluetooth Module: Documentation / Link</li> <li>MAX6675 Thermocouple-to-Digital Converter: Documentation</li> <li>Enclosure: Link</li> <li>SD Card for storing collected data</li> <li>Basic soldering and crimping equipment</li> <li>Double-sided tape to attach components to enclosure</li> </ul>"},{"location":"2.%20Hardware/Weather%20Station/1-%20Weather%20Station%20Build/#build-instructions","title":"Build Instructions","text":"<p>Setup:</p> <ul> <li>Bore a hole in the side of the enclosure to size for a cable gland. Drill the hole as low as possible on the side to allow the Arduino USB cable to be inserted with a minimal bend. Insert a cable gland into the hole. </li> <li>Secure the Arduino and the bluetooth module in the bottom of the enclosure. Secure the thermocouple module (with wire connected) to the inside side of the box opposite the hinge with its pins facing away from the cable gland.  </li> <li>Attach the temperature / humidity sensor to the top of the enclosure if using a radiation shield. If not, attach the sensor on the side of the enclosure.  </li> <li>See bottom of page for the internals of a completed weather station and clarification on placement of components.</li> </ul> <p>Arduino Wiring Guide: </p>"},{"location":"2.%20Hardware/Weather%20Station/1-%20Weather%20Station%20Build/#soldering-guide","title":"Soldering Guide","text":"<ul> <li>Use the example soldered datalogger board (shown below) for reference throughout the soldering process.</li> <li>On three distinct, well-spaced rows of the prototyping area of the datalogger, solder 5-pin, 6-pin, and 3-pin male JST-XH headers. Leave the bottom two rows of the prototyping area untouched. The image below uses rows R1, R5, and R9, leaving R11 free for later. See underside soldering here. </li> <li>Solder a wire from the 5V source to R11 for easier access to the source later on. Solder a wire from the board GND to another point on R11. For convenience, the 5V and GND can also be soldered to the 3-pin header at this time (as shown).</li> <li> <p>NOTE: All pins referred to for soldering on the board reference the digital pins on the right side, not the analog inputs on the left. </p> </li> <li> <p>5-pin header: Connect the first (leftmost) pin to P13. Connect the next pin to P4. Connect the third pin to P12. Connect the fourth pin to the 5V source. Connect the final pin to GND. See soldered board after this step here.</p> </li> <li>6-pin header: The outer pin on either side will be unused. Connect the second pin (leftmost pin in use) to P6. Connect the third pin to P5. Connect the fourth pin to GND. Connect the fifth pin (rightmost pin in use) to the 3.3V source. See soldered board after this step here.</li> <li>3-pin header: Connect the first (leftmost) pin to the 5V source and the last (rightmost) pin to GND if these were not already connected. Connect the second pin to P2.  </li> </ul>"},{"location":"2.%20Hardware/Weather%20Station/1-%20Weather%20Station%20Build/#example-soldered-board","title":"Example Soldered Board","text":"<ul> <li>The final soldered board should look similar to this: </li> </ul>"},{"location":"2.%20Hardware/Weather%20Station/1-%20Weather%20Station%20Build/#assembly","title":"Assembly","text":"<ul> <li>With three distinctly colored wires (sized appropriately to reach the temperature / humidity sensor from the board), use female JST-XH crimp terminals and a crimp tool to crimp the ends of each wire. Connect the ends of each wire to 3-pin female JST-XH Headers. Repeat this process for five wires to length for the thermocouple module.  </li> <li>When connecting the crimped wires to the female headers, pay close attention to how the header will connect with the previously soldered male headers. </li> <li>Using four distinctly colored wires (sized to reach the bluetooth module), connect the ends of each wire to the middle four pins of 6-pin female JST-XH headers.  </li> <li>Connect the female wire headers to the properly sized male headers on the board. The 6-pin connector should be connected to the bluetooth module, with the 5-pin and 3-pin connectors going to the thermocouple module and through the cable gland to the temperature / humidity sensor respectively. See wires correctly connected to the female headers and board here.</li> <li>To ensure that the wires are connected properly, one can match the pin connected to the voltage source with the VCC or GND pin on the respective component. Pay attention to which wire is connected to each pin. </li> <li>Connect the Arduino USB cable through the enclosure. If the cable gland included in the linked enclosure is used, some material will have to be shaved from the corners of the USB-B end of the cable.  </li> <li>Attach the thermocouple wire to the module's input. Depending on the style of thermocouple used, it may be necessary to solder the thermocouple to the module. </li> <li>Attach the datalogger (with a CR1220 battery and SD Card in place) to the Arduino.  </li> <li>If using a radiation shield, attach the radiation shield to the top of the box, covering the temperature / humidity sensor.</li> </ul>"},{"location":"2.%20Hardware/Weather%20Station/1-%20Weather%20Station%20Build/#completed-weather-station","title":"Completed Weather Station","text":"<p>Internals: Station with Radiation Shield: </p>"},{"location":"2.%20Hardware/Weather%20Station/2-%20Weather%20Station%20Use/","title":"Usage Guide","text":""},{"location":"2.%20Hardware/Weather%20Station/2-%20Weather%20Station%20Use/#gemini-weather-station-usage","title":"GEMINI Weather Station Usage","text":""},{"location":"2.%20Hardware/Weather%20Station/2-%20Weather%20Station%20Use/#uploading-rh_temp_logger","title":"Uploading RH_Temp_Logger","text":"<ul> <li>The file RH_Temp_Logger.ino is used to operate the weather station and must be uploaded to the Arduino before use.</li> <li>Install the Arduino IDE software and open RH_Temp_Logger.ino in the IDE.  </li> <li>In the built-in library manager, search for and install: SD, RTClib, DHT sensor library, MAX6675 library, and MAX6675 with hardware SPI.  </li> <li>Connect the Arduino's USB cable to the computer and select the correct board / port in the IDE.</li> <li>Click \"Upload\" (Ctrl+U) to upload the sketch to the Arduino. The station is now ready for use with the GEMINI sensing app.</li> <li>To verify everything is working properly, the serial monitor in the Arduino IDE can be used.</li> </ul> <p>Expected Output </p>"},{"location":"2.%20Hardware/Weather%20Station/2-%20Weather%20Station%20Use/#general-usage","title":"General Usage","text":"<ul> <li>Connect the USB cable from the Arduino to a 5V power supply.  </li> <li>To update the weather station's RTC clock (GMT-0 time zone), turn on the GEMINI app prior to powering the weather station. </li> <li>Depending on your use case, you may either place the weather station in the field to take measurements or attach the unit to the rover.  </li> <li>Data from the weather station is stored in the SD Card on the datalogger. The data is logged in a CSV format in a file titled YYYYMMDD.CSV based on the current date.  </li> </ul>"},{"location":"3.%20Setup/DAS/","title":"DAS","text":""},{"location":"3.%20Setup/DAS/#preparation","title":"Preparation","text":"<p>Laptop, Sabrant DAS, 4 HDDs</p> <p></p> <p>Open the hard disk slots and insert all the hard disks.</p> <p></p> <p>Turn on the power.</p> <p></p> <p>Turn on only the top two for the initial storage pool setup.</p> <p></p>"},{"location":"3.%20Setup/DAS/#data-storage-pool-setup","title":"Data Storage Pool Setup","text":"<p>Set as a Simple type</p> <p></p> <p></p> <p>Next, set the backup disk pool setup</p> <p></p> <p></p> <p>Add Data Disk (D:) to Library</p> <p></p> <p></p> <p>Click (+) New on upper left</p> <p></p> <p>Add D: Drive to the added library</p> <p></p> <p></p> <p>Turn on File History</p> <p></p> <p>Use E: Drive as File History Disk</p> <p>To Test, Copy the file into the D:</p> <p></p> <p>And click run now</p> <p></p>"}]}